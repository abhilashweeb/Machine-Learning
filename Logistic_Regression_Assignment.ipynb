{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory & Practical Questions"
      ],
      "metadata": {
        "id": "J28MxjeAJiEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "* Logistic Regression is a statistical technique used to predict categorical outcomes, particularly binary cases such as yes/no or 0/1. Unlike Linear Regression, which estimates continuous values, Logistic Regression uses the sigmoid (logistic) function to transform outputs into probabilities ranging from 0 to 1, making it well-suited for classification tasks.\n",
        "\n",
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "* In Logistic Regression, the Sigmoid function is used to convert the linear combination of input features (weighted sum plus bias) into an output value between 0 and 1. This value is interpreted as the probability of a data point belonging to a particular class—usually the \"positive\" class in binary classification.\n",
        "* For example, if the output probability is 0.85, it suggests there’s an 85% chance the data point belongs to the positive class. By setting a decision threshold (commonly 0.5), we can classify the point into one of the two categories. This probabilistic approach makes Logistic Regression especially useful in scenarios where we not only want to predict the class but also understand the confidence of that prediction.\n",
        "\n",
        "Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "* Regularization in Logistic Regression is a method used to reduce overfitting by adding a penalty term to the loss function. This penalty discourages the model from assigning excessively large weights to features, thereby keeping the model simpler and more stable. By controlling coefficient magnitudes, regularization improves the model’s ability to generalize and perform well on new, unseen data.\n",
        "\n",
        "Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "* Common evaluation metrics for classification models include the Confusion Matrix, Accuracy, Misclassification Rate, Precision, Recall, F1-Score, and F-Beta Score. These metrics help assess different aspects of model performance, such as how effectively it predicts each class, how well it balances false positives and false negatives, and how it performs when certain types of errors are prioritized. By analyzing these measures, we can identify the most dependable model for the task.\n",
        "\n",
        "Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)\n",
        "\n"
      ],
      "metadata": {
        "id": "BUgPi83QJy7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn (Breast Cancer dataset)\n",
        "data = datasets.load_breast_cancer()\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Create and train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear',max_iter=1000)  # max_iter increased to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wt70nNZLF7x",
        "outputId": "def6a729-14b9-49d1-cb92-acee4f8584ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "YpR0uPqiMDuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn (Breast Cancer dataset)\n",
        "data = datasets.load_breast_cancer()\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Create Logistic Regression model with L2 regularization (default)\n",
        "model = LogisticRegression(solver='liblinear',penalty='l2', max_iter=1000)  # L2 = Ridge regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coeff:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTKlmBFVMTcC",
        "outputId": "1d86db47-b3da-46a5-aced-b13fdd09841c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coeff: [[ 1.82137734  0.07112385 -0.12306166  0.00587456 -0.07595978 -0.32257247\n",
            "  -0.49620872 -0.22286157 -0.11884256 -0.01391574  0.07814557  0.48484736\n",
            "   0.34149697 -0.07132055 -0.01143128 -0.04997087 -0.10056425 -0.03328301\n",
            "  -0.03276574 -0.00328304  1.82550882 -0.23308136 -0.15962553 -0.02971517\n",
            "  -0.13358016 -0.87022834 -1.22100716 -0.41859203 -0.31544016 -0.06521608]]\n",
            "Intercept: [0.3402689]\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report. (Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "l4TG2MW4Myjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load a multiclass dataset from sklearn (Iris dataset)\n",
        "data = datasets.load_iris()\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model for multiclass classification (One-vs-Rest)\n",
        "model = LogisticRegression(solver='liblinear',multi_class='ovr', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB4ke2b0M47A",
        "outputId": "f918d91e-5b38-444e-c2c0-0cc017866ebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "qh_c7j7-NQ5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn (Breast Cancer dataset)\n",
        "data = datasets.load_breast_cancer()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Create Logistic Regression model\n",
        "log_reg = LogisticRegression(solver='liblinear',max_iter=1000)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],       # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],       # Regularization type\n",
        "    'solver': ['liblinear']        # Supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test set using best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print validation accuracy\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWsL_cQxNdMm",
        "outputId": "107edb8c-c2f1-4007-d80e-68ac4e0d756e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Validation Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "54AaWyYjNzuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn (Breast Cancer dataset)\n",
        "data = datasets.load_breast_cancer()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(solver='liblinear',max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_scaling = LogisticRegression(solver='liblinear',max_iter=1000)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", accuracy_scaling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W27IMet7N3Qn",
        "outputId": "3836c9b3-f00a-4c74-cff4-c8759110ca21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "Accuracy with Scaling: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "* If we get this kind of imbalanced dataset, the first step would be data cleaning—removing duplicates, handling missing values through imputation, and correcting errors. Since only 5% of customers respond, the dataset is highly imbalanced, so we’d address this using techniques like oversampling the minority class (e.g., SMOTE) or undersampling the majority class. We could also set class_weight='balanced' in Logistic Regression to make the model penalize mistakes on the minority class more.\n",
        "* Next, we’d scale the features using methods like StandardScaler or MinMaxScaler so that all features contribute equally to the model’s learning. We’d train a Logistic Regression model and tune hyperparameters such as the regularization strength C, penalty type (l1 or l2), and solver to find the best configuration, using cross-validation for reliable results.\n",
        "* For evaluation, instead of relying only on accuracy—which can be misleading for imbalanced data—we’d focus on Precision (avoiding false positives), Recall (capturing as many responders as possible), F1-Score (balancing Precision and Recall), and ROC-AUC (overall discrimination ability). This ensures we not only predict correctly but also identify the customers most likely to respond in line with business goals.\n"
      ],
      "metadata": {
        "id": "rJXD1TT3OCXr"
      }
    }
  ]
}