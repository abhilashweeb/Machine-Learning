{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Code: DA-AG-014\n"
      ],
      "metadata": {
        "id": "_7amoTQYoePi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "* Ensemble Learning is a machine learning technique where multiple models are combined to improve prediction accuracy and reliability. **The idea is that a group of weak models together performs better than a single strong model**. It works by training several models and merging their outputs (through voting, averaging, or stacking) to reduce errors, variance, and bias.\n",
        "* In short: Ensemble learning makes predictions more accurate, stable, and robust.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "* Bagging (Bootstrap Aggregating): Trains models in parallel on random subsets of data. Final prediction is made by averaging or voting. It mainly reduces variance. Example: Random Forest.\n",
        "* Boosting: Trains models sequentially, where each new model fixes the errors of the previous one. Final prediction is a weighted combination of models. It reduces both bias and variance. Example: AdaBoost, XGBoost.\n",
        "* Bagging = parallel, reduces variance; Boosting = sequential, reduces bias & variance.\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "* Bootstrap sampling is a technique where new datasets are created by randomly selecting samples from the original data with replacement. Some records may repeat while others may be left out.\n",
        "* Role in Bagging (e.g., Random Forest)\n",
        "     * Each model is trained on a different bootstrap sample, creating diversity among models. When their outputs are combined (by voting or averaging), the overall prediction becomes more accurate, stable, and less overfitted.\n",
        "* In short: Bootstrap sampling provides varied training sets, which makes Bagging methods like Random Forest effective.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "* In bootstrap sampling, some data points are left out of the sample used to train a model. These unused data points are called Out-of-Bag (OOB) samples.\n",
        "* OOB Score in Ensemble Models:\n",
        "      * OOB samples act like a built-in validation set.\n",
        "      * After training, each model can be tested on its own OOB samples.\n",
        "      * The OOB score is the average accuracy/error computed using these OOB predictions.\n",
        "\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "* Decision Tree: Feature importance = impurity reduction from splits in one tree; can be biased by early/top splits and data quirks.\n",
        "* Random Forest: Importance = average impurity reduction across many trees; more stable, robust, and less biased.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "* Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "* Train a Random Forest Classifier.\n",
        "* Print the top 5 most important features based on feature importance scores.\n",
        "* (Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "drYM3Dezomtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(random_state=1)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=data.feature_names)\n",
        "\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importance.sort_values(ascending=False).head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKFx_RNcyc2V",
        "outputId": "3c780294-7044-4438-977d-06c200757e3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst concave points    0.123350\n",
            "worst perimeter         0.115661\n",
            "worst area              0.105248\n",
            "worst radius            0.102798\n",
            "mean concave points     0.100735\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "* Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "* Evaluate its accuracy and compare with a single Decision Tree\n",
        "* (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "NQBEnbDlyi66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=1)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=1)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ5_DPivyuOG",
        "outputId": "198d2657-fa4d-4c98-ca7c-90cc7afa451c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9555555555555556\n",
            "Bagging Classifier Accuracy: 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "* Train a Random Forest Classifier\n",
        "* Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "* Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "-3vru1fqy1Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=1)\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [20, 50, 100],\n",
        "    \"max_depth\": [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMv-yMaNzBMe",
        "outputId": "8b095a5f-dab0-455e-9831-b87993d04fef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "* Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "* Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "IgWO8cLWzVVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=1)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=50, random_state=1)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CC1G7elzeQi",
        "outputId": "0def86ac-ec92-4212-8cc4-27119919ed41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.26483002536121963\n",
            "Random Forest Regressor MSE: 0.26582715600876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "* Choose between Bagging or Boosting\n",
        "* Handle overfitting\n",
        "* Select base models\n",
        "* Evaluate performance using cross-validation\n",
        "* Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer**-\n",
        "* Step 1: Choose between Bagging or Boosting We first check if our dataset has high variance or high bias. If the model tends to overfit (high variance), we prefer Bagging like Random Forest. If the model underfits and misses patterns, we use Boosting like XGBoost or AdaBoost to focus on hard-to-predict cases.\n",
        "* Step 2: Handle overfitting We use techniques like limiting tree depth, reducing the number of estimators, and applying regularization in boosting. We also use cross-validation to tune parameters and ensure the model performs well on unseen data.\n",
        "* Step 3: Select base models We usually start with Decision Trees because they are simple and work well in ensembles. For more complex data, we can also try logistic regression or shallow models as base learners depending on the problem.\n",
        "* Step 4: Evaluate performance using cross-validation We split the dataset into multiple folds and train/test on different parts. This gives us an average performance score that is more reliable than a single train-test split. It helps us choose the best ensemble setup.\n",
        "* Step 5: Justify ensemble learning in real-world context In loan default prediction, wrong decisions can be very costly. Ensemble methods combine multiple models, making predictions more stable and accurate. This reduces both false approvals and false rejections, helping the financial institution make safer and smarter lending decisions.\n",
        "* By carefully choosing the right ensemble method, tuning parameters, and validating results, we build a strong predictive model. Ensemble learning reduces errors and provides more reliable insights, which is crucial for making better financial decisions like predicting loan defaults."
      ],
      "metadata": {
        "id": "KGfqBeXlzp06"
      }
    }
  ]
}