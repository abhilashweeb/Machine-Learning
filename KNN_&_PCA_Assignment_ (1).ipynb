{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Code: DA-AG-016"
      ],
      "metadata": {
        "id": "ElkYmEv_odD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "* K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It is called a “lazy learner” because it does not explicitly build a model; instead, it makes predictions based on the entire training dataset.\n",
        "* How it works;\n",
        "     * For a new data point, find the K nearest neighbors using distance (like Euclidean distance).\n",
        "     * Use those neighbors to make the prediction.\n",
        "* KNN in Classification;\n",
        "     * The algorithm uses a majority vote among the K neighbors.\n",
        "     * Example: If K=7 and among the 7 closest neighbors, 4 belong to class A and 3 to class B, the new point is classified as class A.\n",
        "* KNN in Regression;\n",
        "     * Instead of voting, KNN takes the average (or weighted average) of the target values of the K nearest neighbors.\n",
        "     * Example: If the target values of the K neighbors are [10, 12, 14], then the predicted value is their average (≈ 12).\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "* The **curse of dimensionality** refers to the problems that arise when working with data that has a very large number of features (high dimensions).\n",
        "* As dimensions increased;\n",
        "        * Data points become sparser (spread out).\n",
        "        * Distances between points become less meaningful.\n",
        "        * Models that rely on distance (like KNN) struggle to find “true” nearest neighbors.\n",
        "* Effect on KNN;\n",
        "\n",
        "        Since KNN depends on distance:\n",
        "        * Nearest neighbors become hard to identify.\n",
        "        * Accuracy drops.\n",
        "        * Computation cost increases.\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "* PCA is a dimensionality reduction technique that transforms the original features into a new set of features caled principal components.\n",
        "    * These components are linear combinations of the original features.\n",
        "    * They capture the maximum variance in the data with fewer dimensions.\n",
        "\n",
        "* Difference from Feature Selection\n",
        "    * Feature Selection: Keeps the most important original features.\n",
        "    * PCA (Feature Extraction): Creates new features by combining original ones.\n",
        "\n",
        "* So basically, PCA transforms features, while feature selection just picks features.\n",
        "\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "* Eigenvectors: Directions in which the data varies the most (they define the principal components).\n",
        "* Eigenvalues: Numbers that show how much variance (information) each eigenvector carries.\\\n",
        "* Their Importance;\n",
        "      * Eigenvectors decide the new feature axes (principal components).\n",
        "      * Eigenvalues tell us the importance of each component, helping us choose how many components to keep.\n",
        "* **Eigenvectors** give the directions of maximum variance, and **Eigenvalues** tell the amount of variance captured. Together, they form the core of PCA.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "* KNN and PCA\n",
        "    * Problem: KNN struggles with high-dimensional data because of the curse of dimensionality.\n",
        "    * Solution: PCA reduces dimensions by keeping only the most important information.\n",
        "\n",
        "* How they complement each other?\n",
        "    * PCA transforms high-dimensional data into a lower-dimensional space.\n",
        "    * KNN is then applied on this compact data, where distances are more reliable.\n",
        "    * This makes KNN faster, less memory-intensive, and more accurate.\n",
        "\n",
        "* Example;\n",
        "     In image recognition, each image may have thousands of pixel features.\n",
        "       * PCA reduces dimensions to a smaller set of components (e.g., 100 instead of 10,000 pixels).\n",
        "       * KNN then classifies images using these reduced features, improving both speed and performance.\n",
        "\n",
        "\n",
        "**Dataset**:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "rKPC4tSppeAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# --- KNN without scaling ---\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "acc_no_scaling = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "# --- KNN with scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_with_scaling = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy without scaling:\", round(acc_no_scaling, 2))\n",
        "print(\"Accuracy with scaling:\", round(acc_with_scaling, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvs1pwZauvV9",
        "outputId": "53094aa3-a615-442d-9e76-43c137b67f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.74\n",
            "Accuracy with scaling: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "ECUWccrbvRjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X, _ = load_wine(return_X_y=True)\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "pca = PCA().fit(X_scaled)\n",
        "\n",
        "print(\"Explained variance ratio of each component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {var:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsBYLrsQvYiv",
        "outputId": "c14fdb60-91a1-4882-faa3-21666551fe7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "N4Pu-YSIwClT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled, X_test_scaled = scaler.fit_transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_original = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca, X_test_pca = pca.fit_transform(X_train_scaled), pca.transform(X_test_scaled)\n",
        "\n",
        "knn.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn.predict(X_test_pca))\n",
        "\n",
        "print(\"Accuracy on original dataset:\", round(acc_original, 2))\n",
        "print(\"Accuracy on PCA-transformed dataset (2 components):\", round(acc_pca, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-WKLv7NwJdC",
        "outputId": "d750d74d-1d44-4b4c-b9e1-d78be0513f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.96\n",
            "Accuracy on PCA-transformed dataset (2 components): 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "7g1be_riwtfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled, X_test_scaled = scaler.fit_transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test_scaled))\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy with Euclidean distance:\", round(acc_euclidean, 2))\n",
        "print(\"Accuracy with Manhattan distance:\", round(acc_manhattan, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AwZ6c1kwwd-",
        "outputId": "58fcbf96-1ba7-4b66-9fa1-052b6108c6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.96\n",
            "Accuracy with Manhattan distance: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "Explain how you would:\n",
        "* Use PCA to reduce dimensionality\n",
        "* Decide how many components to keep\n",
        "* Use KNN for classification post-dimensionality reduction\n",
        "* Evaluate the model\n",
        "* Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer**-\n",
        "* The Process\n",
        "      * Use PCA → reduce dimensions of high-dimensional gene expression dataset.\n",
        "      * Decide components → keep enough components to explain ~95% variance.\n",
        "      * Apply KNN → for classification after dimensionality reduction.\n",
        "      * Evaluate → check accuracy with train-test split.\n",
        "      * Justify → PCA+KNN avoids overfitting and is efficient for biomedical data.\n"
      ],
      "metadata": {
        "id": "TyRq45kAxJW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulate a high-dimensional gene expression dataset\n",
        "X, y = make_classification(n_samples=200, n_features=1000, n_informative=50,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply PCA to keep 95% variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = knn.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Original features:\", X.shape[1])\n",
        "print(\"Reduced features after PCA:\", X_pca.shape[1])\n",
        "print(\"Model Accuracy:\", round(acc, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ET77yw-xuKN",
        "outputId": "11304d4d-7824-4155-8a40-0fc0a756f1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features: 1000\n",
            "Reduced features after PCA: 175\n",
            "Model Accuracy: 0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MvYaKxAKyQfj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}